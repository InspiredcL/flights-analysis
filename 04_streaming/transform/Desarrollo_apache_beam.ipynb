{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo Apache Beam\n",
    "\n",
    "Pasos a seguir para desarrollar los scripts, es redundante pero asi se aprende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalamos ipykernel, si es que no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalamos las dependencias de apache beam, en este caso el runner interactivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install apache-beam[interactive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los archivos necesarios para trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Airports\n",
    "\n",
    "Paso 1 para procesar los aeropuertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    airports = (pipeline\n",
    "                | beam.io.ReadFromText('airports_2024.csv.gz')\n",
    "                | beam.Map(lambda line: next(csv.reader([line])))\n",
    "                | beam.Map(lambda fields: (fields[0], (fields[21], fields[26])))\n",
    "                )\n",
    "\n",
    "    transformed_airports = (airports\n",
    "                            | beam.Map(lambda airport_data: '{},{}'.format(\n",
    "                                airport_data[0], ','.join(airport_data[1])))\n",
    "                            | beam.io.WriteToText('df01_extracted_airports')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Opcional]\\\n",
    "Antes de ejecutar el paso 2 comprobamos la cantidad de aeropuertos\n",
    "en Estados Unidos\\\n",
    "Aprox. 7288 cumplen con\\\n",
    "AIRPORT_COUNTRY_NAME=='United States'\\\n",
    "o por numero de columna\\\n",
    "8=='United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    us_airports = (pipeline\n",
    "                | beam.io.ReadFromText('airports_2024.csv.gz')\n",
    "                | beam.Filter(lambda line: \"United States\" in line)\n",
    "                | beam.Map(lambda line: next(csv.reader([line])))\n",
    "                | beam.Map(lambda fields: (fields[0], (fields[21], fields[26])))\n",
    "                )\n",
    "\n",
    "    transformed_airports = (us_airports\n",
    "                            | beam.Map(lambda airport_data: '{},{}'.format(\n",
    "                                airport_data[0], ','.join(airport_data[1])))\n",
    "                            | beam.io.WriteToText('df01_extracted_us_airports')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Opcional]\\\n",
    "Antes de ejecutar el paso 2 comprobamos la cantidad de aeropuertos\n",
    "en Estados Unidos y los aeropuertos actuales\\\n",
    "Aprox. 2926 cumplen con AIRPORT_COUNTRY_NAME=='United States' y\n",
    "AIRPORT_IS_LATEST=='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    last_us_airports = (pipeline\n",
    "                        | beam.io.ReadFromText('airports_2024.csv.gz')\n",
    "                        | beam.Filter(\n",
    "                            lambda line: \"United States\" in line and line[-1:] == '1'\n",
    "                        )  # no se si va acá o identado\n",
    "                        | beam.Map(lambda line: next(csv.reader([line])))\n",
    "                        | beam.Map(lambda fields: (fields[0], (fields[21], fields[26])))\n",
    "                        )\n",
    "\n",
    "    transformed_airports = (last_us_airports\n",
    "                            | beam.Map(lambda airport_data: '{},{}'.format(\n",
    "                                airport_data[0], ','.join(airport_data[1])))\n",
    "                            | beam.io.WriteToText(\n",
    "                                'df01_extracted_last_us_airports'\n",
    "                            )\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los archivos procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm df01_extracted_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs. Podríamos considerar la fecha de funcionamiento del aeropuerto y las\n",
    "fechas del vuelo en nuestro pipeline eso seria mas correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Airports\n",
    "Importamos lo necesario para continuar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timezonefinder\n",
    "from pytz.exceptions import UnknownTimeZoneError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2 para procesar los aeropuertos\\\n",
    "Obs. Duración local 30 min aprox con 7288 filas aprox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtimezone(lat, lon):\n",
    "    \"\"\"Agrega zona horaria\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Creamos una instancia de la clase para que sea re-usada\n",
    "        tf = timezonefinder.TimezoneFinder()\n",
    "        # Comprobar en qué zona horaria se encuentra un punto\n",
    "        tz = tf.timezone_at(lng=float(lon), lat=float(lat))\n",
    "        if tz is None:\n",
    "            tz = 'UTC'\n",
    "        return lat, lon, tz\n",
    "    except (ValueError, UnknownTimeZoneError):\n",
    "        return lat, lon, 'TIMEZONE'  # Encabezado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    airports = (pipeline\n",
    "                | beam.io.ReadFromText('airports_2024.csv.gz')\n",
    "                | beam.Filter(lambda line: \"United States\" in line)\n",
    "                | beam.Map(lambda line: next(csv.reader([line])))\n",
    "                | beam.Map(\n",
    "                    lambda fields: (\n",
    "                        fields[0], addtimezone(fields[21], fields[26])\n",
    "                    )\n",
    "                )\n",
    "                )\n",
    "\n",
    "    airports_with_tz = (airports\n",
    "                        | beam.Map(lambda f: f\"{f[0]},{','.join(f[1])}\")\n",
    "                        | beam.io.textio.WriteToText(\n",
    "                            'df02_airports_with_tz'\n",
    "                        )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los archivos procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm df02_airports_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Airports\n",
    "Importamos lo necesario para continuar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones a usar en el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtimezone(lat, lon):\n",
    "    \"\"\"\n",
    "    Agrega la zona horaria correspondiente a las coordenadas proporcionadas.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Crear una instancia de TimezoneFinder\n",
    "        tf = timezonefinder.TimezoneFinder()\n",
    "        # Convertir las coordenadas a números de punto flotante\n",
    "        lat = float(lat)\n",
    "        lon = float(lon)\n",
    "        # Devolver las coordenadas y la zona horaria correspondiente\n",
    "        return lat, lon, tf.timezone_at(lng=lon, lat=lat)\n",
    "    except (ValueError, UnknownTimeZoneError):\n",
    "        # Manejo de excepción en caso de error de valor o timezone\n",
    "        return lat, lon, 'TIMEZONE'  # Encabezado\n",
    "\n",
    "\n",
    "def as_utc(date, hhmm, tzone):\n",
    "    \"\"\" Convierte a UTC.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Verifica si hay una hora válida y una zona horaria proporcionada\n",
    "        if len(hhmm) > 0 and tzone is not None:\n",
    "            # Para manejar fechas y zonas horarias\n",
    "            # Crea un objeto de zona horaria utilizando la zona proporcionada\n",
    "            loc_tz = pytz.timezone(tzone)\n",
    "            # Convierte la fecha en un objeto datetime en la zona horaria local\n",
    "            loc_dt = loc_tz.localize(\n",
    "                datetime.datetime.strptime(date, '%Y-%m-%d'),\n",
    "                is_dst=False\n",
    "            )\n",
    "            # La hora se divide en horas y minutos, y se agrega a la fecha y hora local\n",
    "            loc_dt += datetime.timedelta(\n",
    "                hours=int(hhmm[:2]),\n",
    "                minutes=int(hhmm[2:])\n",
    "            )\n",
    "            # Convierte la fecha y hora local en UTC\n",
    "            utc_dt = loc_dt.astimezone(pytz.utc)\n",
    "            # Retorna la fecha y hora en formato de cadena 'YYYY-MM-DD HH:MM:SS'\n",
    "            return utc_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            # Si no hay hora válida o zona horaria, retorna una cadena vacía\n",
    "            return ''  # Una cadena vacía corresponde a vuelos cancelados\n",
    "    except ValueError as e:\n",
    "        # Si ocurre un error de ValueError, registra la excepción y vuelve a lanzarla\n",
    "        logging.exception(\"%s %s %s ValueError: %s\", date, hhmm, tzone, e)\n",
    "\n",
    "\n",
    "def tz_correct(line, airport_timezones):\n",
    "    \"\"\" Correcciones de zonas horarias.\"\"\"\n",
    "\n",
    "    # Cargamos los campos del registro JSON en un diccionario llamado \"fields\".\n",
    "    fields = json.loads(line)\n",
    "    try:\n",
    "        # Obtenemos el ID del aeropuerto de origen y destino.\n",
    "        dep_airport_id = fields[\"ORIGIN_AIRPORT_SEQ_ID\"]\n",
    "        arr_airport_id = fields[\"DEST_AIRPORT_SEQ_ID\"]\n",
    "        # Obtenemos las zonas horarias de los aeropuertos de origen y destino.\n",
    "        dep_timezone = airport_timezones[dep_airport_id][2]\n",
    "        arr_timezone = airport_timezones[arr_airport_id][2]\n",
    "        # Iteramos sobre las hhmm de embarque y las convertimos a UTC.\n",
    "        for f in [\"CRS_DEP_TIME\", \"DEP_TIME\", \"WHEELS_OFF\"]:\n",
    "            fields[f] = as_utc(fields[\"FL_DATE\"], fields[f], dep_timezone)\n",
    "        # Iteramos sobre las hhmm de llegada y las convertimos a UTC.\n",
    "        for f in [\"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f] = as_utc(fields[\"FL_DATE\"], fields[f], arr_timezone)\n",
    "        # Generamos una cadena JSON con los campos actualizados y la devolvemos.\n",
    "        yield json.dumps(fields)\n",
    "    except KeyError:\n",
    "        # En caso de que falte una clave en el diccionario, registramos una excepción.\n",
    "        logging.exception(\n",
    "            \" Ignorando %s porque el aeropuerto no es conocido\",\n",
    "            line\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    # Leer el archivo 'airports.csv.gz' y filtrar líneas con \"United States\"\n",
    "    airports = (pipeline\n",
    "                | 'airports:read' >> beam.io.ReadFromText(\n",
    "                    'airports_2024.csv.gz')\n",
    "                | beam.Filter(lambda line: \"United States\" in line)\n",
    "                # Mapear cada línea a los campos correspondientes\n",
    "                | 'airports:fields' >> beam.Map(\n",
    "                    lambda line: next(\n",
    "                        csv.reader([line])\n",
    "                    )\n",
    "                )\n",
    "                # Mapear los campos para agregar la zona horaria\n",
    "                | 'airports:tz' >> beam.Map(\n",
    "                    lambda fields: (\n",
    "                        fields[0], addtimezone(fields[21], fields[26])\n",
    "                    )\n",
    "                )\n",
    "                )\n",
    "    logging.info(\"Éxito en airports:tz\")\n",
    "\n",
    "    # Leer el archivo 'flights_sample.json' y realizar corrección de zona horaria\n",
    "    flights = (pipeline\n",
    "               | 'flights:read' >> beam.io.ReadFromText(\n",
    "                   'flights_sample_2024.json'\n",
    "               )\n",
    "               | 'flights:tzcorr' >> beam.FlatMap(\n",
    "                   tz_correct, beam.pvalue.AsDict(airports)\n",
    "               )\n",
    "               )\n",
    "    logging.info(\"Éxito en flights:tzcorr\")\n",
    "\n",
    "    # Escribir los resultados en un archivo 'all_flights'\n",
    "    all_flights = (flights\n",
    "                   | beam.io.textio.WriteToText('df03_all_flights')\n",
    "                   )\n",
    "    logging.info(\"Éxito en escribir df03_all_flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- dwa\n",
    "\n",
    "vdsavdsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportOptionalMemberAccess=false\n",
    "\n",
    "\n",
    "def addtimezone(lat, lon):\n",
    "    \"\"\"\n",
    "    Agrega la zona horaria correspondiente a las coordenadas proporcionadas.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        tf = timezonefinder.TimezoneFinder()\n",
    "        lat = float(lat)\n",
    "        lon = float(lon)\n",
    "        return lat, lon, tf.timezone_at(lng=lon, lat=lat)\n",
    "    except (ValueError, UnknownTimeZoneError):\n",
    "        return lat, lon, 'TIMEZONE'  # header\n",
    "\n",
    "\n",
    "def as_utc(date, hhmm, tzone):\n",
    "    \"\"\"Convierte una fecha y hora en formato UTC\"\"\"\n",
    "\n",
    "    try:\n",
    "        if len(hhmm) > 0 and tzone is not None:\n",
    "            loc_tz = pytz.timezone(tzone)\n",
    "            loc_dt = loc_tz.localize(\n",
    "                datetime.datetime.strptime(date, '%Y-%m-%d'),\n",
    "                is_dst=False\n",
    "            )\n",
    "            # Considera las horas 2400 y 0000\n",
    "            loc_dt += datetime.timedelta(\n",
    "                hours=int(hhmm[:2]), minutes=int(hhmm[2:]))\n",
    "            utc_dt = loc_dt.astimezone(pytz.utc)\n",
    "            return (\n",
    "                utc_dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                loc_dt.utcoffset().total_seconds()\n",
    "            )\n",
    "        else:\n",
    "            # Vuelos cancelados y offset de 0\n",
    "            print(\"Devolviendo ('', 0) porque hhmm está vacío or tzone es None\")\n",
    "            return '', 0\n",
    "    except ValueError as e:\n",
    "        logging.exception(\"%s %s %s ValueError: %s\", date, hhmm, tzone, e)\n",
    "        print(\"Exception occurred in as_utc:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_24h_if_before(arrtime, deptime):\n",
    "    \"\"\"add_24h_if_before\"\"\"\n",
    "\n",
    "    if len(arrtime) > 0 and len(deptime) > 0 and arrtime < deptime:\n",
    "        adt = datetime.datetime.strptime(arrtime, '%Y-%m-%d %H:%M:%S')\n",
    "        adt += datetime.timedelta(hours=24)\n",
    "        return adt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        return arrtime\n",
    "\n",
    "\n",
    "def tz_correct(line, airport_timezones):\n",
    "    \"\"\"Realiza un ajuste de zonas horarias\"\"\"\n",
    "\n",
    "    fields = json.loads(line)\n",
    "    try:\n",
    "        # convert all times to UTC\n",
    "        dep_airport_id = fields[\"ORIGIN_AIRPORT_SEQ_ID\"]\n",
    "        arr_airport_id = fields[\"DEST_AIRPORT_SEQ_ID\"]\n",
    "        dep_timezone = airport_timezones[dep_airport_id][2]\n",
    "        arr_timezone = airport_timezones[arr_airport_id][2]\n",
    "\n",
    "        for f in [\"CRS_DEP_TIME\", \"DEP_TIME\", \"WHEELS_OFF\"]:\n",
    "            fields[f], deptz = as_utc(\n",
    "                fields[\"FL_DATE\"],\n",
    "                fields[f],\n",
    "                dep_timezone\n",
    "            )  # type: ignore\n",
    "\n",
    "        for f in [\"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f], arrtz = as_utc(\n",
    "                fields[\"FL_DATE\"],\n",
    "                fields[f],\n",
    "                arr_timezone\n",
    "            )  # type: ignore\n",
    "\n",
    "        for f in [\"WHEELS_OFF\", \"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f] = add_24h_if_before(\n",
    "                fields[f],\n",
    "                fields[\"DEP_TIME\"]\n",
    "            )\n",
    "\n",
    "        fields[\"DEP_AIRPORT_LAT\"] = airport_timezones[dep_airport_id][0]\n",
    "        fields[\"DEP_AIRPORT_LON\"] = airport_timezones[dep_airport_id][1]\n",
    "        fields[\"DEP_AIRPORT_TZOFFSET\"] = deptz\n",
    "        fields[\"ARR_AIRPORT_LAT\"] = airport_timezones[arr_airport_id][0]\n",
    "        fields[\"ARR_AIRPORT_LON\"] = airport_timezones[arr_airport_id][1]\n",
    "        fields[\"ARR_AIRPORT_TZOFFSET\"] = arrtz\n",
    "        yield json.dumps(fields)\n",
    "    except KeyError as e:\n",
    "        # En caso de que falte una clave en el diccionario, registramos una excepción.\n",
    "        logging.exception(\n",
    "            \" Ignorando %s aeropuerto no conocido, KeyError Error: %s\",\n",
    "            line,\n",
    "            e\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline('DirectRunner') as pipeline:\n",
    "    airports = (pipeline\n",
    "                | 'airports:read' >> beam.io.ReadFromText(\n",
    "                    'airports_2024.csv.gz'\n",
    "                )\n",
    "                | beam.Filter(lambda line: \"United States\" in line)\n",
    "                | 'airports:fields' >> beam.Map(\n",
    "                    lambda line: next(\n",
    "                        csv.reader([line])\n",
    "                    )\n",
    "                )\n",
    "                | 'airports:tz' >> beam.Map(\n",
    "                    lambda fields: (\n",
    "                        fields[0],\n",
    "                        addtimezone(fields[21], fields[26])\n",
    "                    )\n",
    "                )\n",
    "                )\n",
    "    flights = (pipeline\n",
    "                | 'flights:read' >> beam.io.ReadFromText(\n",
    "                    'flights_sample_2024.json'\n",
    "                )\n",
    "                | 'flights:tzcorr' >> beam.FlatMap(\n",
    "                    tz_correct,\n",
    "                    beam.pvalue.AsDict(\n",
    "                        airports)  # type: ignore\n",
    "                )\n",
    "                )\n",
    "\n",
    "    all_flights = (flights\n",
    "                    | beam.io.textio.WriteToText('df04_all_flights')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- fewfe\n",
    "\n",
    "fwefewfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportGeneralTypeIssues =false\n",
    "def tz_correct(fields, airport_timezones):\n",
    "    \"\"\"\n",
    "    Realiza un ajuste de zonas horarias para los campos de fecha y hora\"\"\"\n",
    "\n",
    "    try:\n",
    "        # convert all times to UTC\n",
    "        dep_airport_id = fields[\"ORIGIN_AIRPORT_SEQ_ID\"]\n",
    "        arr_airport_id = fields[\"DEST_AIRPORT_SEQ_ID\"]\n",
    "        dep_timezone = airport_timezones[dep_airport_id][2]\n",
    "        arr_timezone = airport_timezones[arr_airport_id][2]\n",
    "\n",
    "        for f in [\"CRS_DEP_TIME\", \"DEP_TIME\", \"WHEELS_OFF\"]:\n",
    "            fields[f], deptz = as_utc(fields[\"FL_DATE\"], fields[f], dep_timezone)\n",
    "        for f in [\"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f], arrtz = as_utc(fields[\"FL_DATE\"], fields[f], arr_timezone)\n",
    "\n",
    "        for f in [\"WHEELS_OFF\", \"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f] = add_24h_if_before(fields[f], fields[\"DEP_TIME\"])\n",
    "\n",
    "        fields[\"DEP_AIRPORT_LAT\"] = airport_timezones[dep_airport_id][0]\n",
    "        fields[\"DEP_AIRPORT_LON\"] = airport_timezones[dep_airport_id][1]\n",
    "        fields[\"DEP_AIRPORT_TZOFFSET\"] = deptz\n",
    "        fields[\"ARR_AIRPORT_LAT\"] = airport_timezones[arr_airport_id][0]\n",
    "        fields[\"ARR_AIRPORT_LON\"] = airport_timezones[arr_airport_id][1]\n",
    "        fields[\"ARR_AIRPORT_TZOFFSET\"] = arrtz\n",
    "        yield fields\n",
    "    except KeyError as e:\n",
    "        # En caso de que falte una clave en el diccionario, registramos una excepción.\n",
    "        logging.exception(\n",
    "            \" Ignorando %s aeropuerto no conocido, KeyError Error: %s\", fields, e\n",
    "        )\n",
    "\n",
    "\n",
    "def get_next_event(fields):\n",
    "    \"\"\"Determina el siguiente evento a partir de los campos disponibles.\"\"\"\n",
    "\n",
    "    if len(fields[\"DEP_TIME\"]) > 0:\n",
    "        event = dict(fields)  # copia de linea json\n",
    "        event[\"EVENT_TYPE\"] = \"departed\"\n",
    "        event[\"EVENT_TIME\"] = fields[\"DEP_TIME\"]\n",
    "        for f in [\n",
    "            \"TAXI_OUT\",\n",
    "            \"WHEELS_OFF\",\n",
    "            \"WHEELS_ON\",\n",
    "            \"TAXI_IN\",\n",
    "            \"ARR_TIME\",\n",
    "            \"ARR_DELAY\",\n",
    "            \"DISTANCE\",\n",
    "        ]:\n",
    "            event.pop(f, None)  # No se conoce el dato a la hora de embarque\n",
    "        yield event\n",
    "    if len(fields[\"ARR_TIME\"]) > 0:\n",
    "        event = dict(fields)\n",
    "        event[\"EVENT_TYPE\"] = \"arrived\"\n",
    "        event[\"EVENT_TIME\"] = fields[\"ARR_TIME\"]\n",
    "        yield event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "with beam.Pipeline(\"DirectRunner\") as pipeline:\n",
    "    airports = (\n",
    "        pipeline\n",
    "        | \"airports:read\" >> beam.io.ReadFromText(\"airports_2024.csv.gz\")\n",
    "        | beam.Filter(lambda line: \"United States\" in line)\n",
    "        | \"airports:fields\" >> beam.Map(lambda line: next(csv.reader([line])))\n",
    "        | \"airports:tz\"\n",
    "        >> beam.Map(lambda fields: (fields[0], addtimezone(fields[21], fields[26])))\n",
    "    )\n",
    "\n",
    "    flights = (\n",
    "        pipeline\n",
    "        | \"flights:read\" >> beam.io.ReadFromText(\"flights_sample_2024.json\")\n",
    "        | \"flights:parse\" >> beam.Map(lambda line: json.loads(line))\n",
    "        | \"flights:tzcorr\" >> beam.FlatMap(tz_correct, beam.pvalue.AsDict(airports))\n",
    "    )\n",
    "\n",
    "    write_flights = (\n",
    "        flights\n",
    "        | \"flights:tostring\" >> beam.Map(lambda fields: json.dumps(fields))\n",
    "        | \"flights:out\" >> beam.io.textio.WriteToText(\"df_05_all_flights\")\n",
    "    )\n",
    "\n",
    "    events = flights | beam.FlatMap(get_next_event)\n",
    "\n",
    "    print_events = (\n",
    "        events\n",
    "        | \"events:tostring\" >> beam.Map(lambda fields: json.dumps(fields))\n",
    "        | \"events:out\" >> beam.io.textio.WriteToText(\"df05_all_events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- BigQuery\n",
    "\n",
    "Leer y escribir desde bigquery necesita un bucket, ya que el SDK de python\n",
    "para apache beam invoca una solicitud de exportación cuando aplicamos\n",
    "una transformación de lectura BigQueryIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportGeneralTypeIssues =false\n",
    "\n",
    "def tz_correct(fields, airport_timezones):\n",
    "    \"\"\"\n",
    "    Realiza un ajuste de zonas horarias para los campos de fecha y hora\n",
    "    \"\"\"\n",
    "\n",
    "    # Compatibilidad con JSON y BigQuery\n",
    "    fields['FL_DATE'] = fields['FL_DATE'].strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        # Convertir a UTC\n",
    "        dep_airport_id = fields[\"ORIGIN_AIRPORT_SEQ_ID\"]\n",
    "        arr_airport_id = fields[\"DEST_AIRPORT_SEQ_ID\"]\n",
    "        dep_timezone = airport_timezones[dep_airport_id][2]\n",
    "        arr_timezone = airport_timezones[arr_airport_id][2]\n",
    "        for f in [\"CRS_DEP_TIME\", \"DEP_TIME\", \"WHEELS_OFF\"]:\n",
    "            fields[f], deptz = as_utc(\n",
    "                fields[\"FL_DATE\"], fields[f], dep_timezone)\n",
    "        for f in [\"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f], arrtz = as_utc(\n",
    "                fields[\"FL_DATE\"], fields[f], arr_timezone)\n",
    "        for f in [\"WHEELS_OFF\", \"WHEELS_ON\", \"CRS_ARR_TIME\", \"ARR_TIME\"]:\n",
    "            fields[f] = add_24h_if_before(fields[f], fields[\"DEP_TIME\"])\n",
    "\n",
    "        fields[\"DEP_AIRPORT_LAT\"] = airport_timezones[dep_airport_id][0]\n",
    "        fields[\"DEP_AIRPORT_LON\"] = airport_timezones[dep_airport_id][1]\n",
    "        fields[\"DEP_AIRPORT_TZOFFSET\"] = deptz\n",
    "        fields[\"ARR_AIRPORT_LAT\"] = airport_timezones[arr_airport_id][0]\n",
    "        fields[\"ARR_AIRPORT_LON\"] = airport_timezones[arr_airport_id][1]\n",
    "        fields[\"ARR_AIRPORT_TZOFFSET\"] = arrtz\n",
    "        yield fields\n",
    "    except KeyError:\n",
    "        logging.exception(\n",
    "            \"Ignoring %s because airport is not known,error: %s\",\n",
    "            fields,\n",
    "            KeyError\n",
    "        )\n",
    "\n",
    "\n",
    "def get_next_event(fields):\n",
    "    \"\"\"\n",
    "    Determina el siguiente evento de un vuelo a partir de los campos de datos\n",
    "    \"\"\"\n",
    "\n",
    "    if len(fields[\"DEP_TIME\"]) > 0:\n",
    "        event = dict(fields)  # copy\n",
    "        event[\"EVENT_TYPE\"] = \"departed\"\n",
    "        event[\"EVENT_TIME\"] = fields[\"DEP_TIME\"]\n",
    "        for f in [\"TAXI_OUT\", \"WHEELS_OFF\", \"WHEELS_ON\", \"TAXI_IN\",\n",
    "                  \"ARR_TIME\", \"ARR_DELAY\", \"DISTANCE\"]:\n",
    "            event.pop(f, None)  # not knowable at departure time\n",
    "        yield event\n",
    "    if len(fields[\"WHEELS_OFF\"]) > 0:\n",
    "        event = dict(fields)  # copy\n",
    "        event[\"EVENT_TYPE\"] = \"wheelsoff\"\n",
    "        event[\"EVENT_TIME\"] = fields[\"WHEELS_OFF\"]\n",
    "        for f in [\"WHEELS_ON\", \"TAXI_IN\", \"ARR_TIME\", \"ARR_DELAY\", \"DISTANCE\"]:\n",
    "            event.pop(f, None)  # not knowable at departure time\n",
    "        yield event\n",
    "    if len(fields[\"ARR_TIME\"]) > 0:\n",
    "        event = dict(fields)\n",
    "        event[\"EVENT_TYPE\"] = \"arrived\"\n",
    "        event[\"EVENT_TIME\"] = fields[\"ARR_TIME\"]\n",
    "        yield event\n",
    "\n",
    "\n",
    "# Crea fila bq\n",
    "def create_event_row(fields):\n",
    "    \"\"\"Crea una fila de evento para ser utilizada en un formato tabular.\"\"\"\n",
    "\n",
    "    featdict = dict(fields)  # copy\n",
    "    featdict['EVENT_DATA'] = json.dumps(fields)\n",
    "    return featdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportPrivateImportUsage=false\n",
    "# pyright: reportAttributeAccessIssue=false\n",
    "# pyright: reportUnusedExpression =false\n",
    "\n",
    "def run(project):\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline con los argumentos correspondientes\n",
    "    \"\"\"\n",
    "\n",
    "    argv = [\n",
    "        f'--project={project}',\n",
    "        '--runner=DirectRunner'\n",
    "    ]\n",
    "    airports_query = f'SELECT * FROM {project}.dsongcp.airports'\n",
    "    sample_query = f'SELECT * FROM {project}.dsongcp.flights_sample'\n",
    "    flights_output = 'df06_all_flights'\n",
    "\n",
    "    with beam.Pipeline(argv=argv) as pipeline:\n",
    "        airports = (pipeline\n",
    "                    | 'airports:read' >> beam.io.ReadFromBigQuery(\n",
    "                        query=airports_query,\n",
    "                        use_standard_sql=True\n",
    "                    )\n",
    "                    | beam.Filter(lambda line: \"United States\" in line)\n",
    "                    | 'airports:fields' >> beam.Map(\n",
    "                        lambda line: next(csv.reader([line]))\n",
    "                    )\n",
    "                    | 'airports:tz' >> beam.Map(\n",
    "                        lambda fields: (fields[0],\n",
    "                                        addtimezone(fields[21], fields[26])\n",
    "                                        )\n",
    "                    )\n",
    "                    )\n",
    "\n",
    "        flights = (pipeline\n",
    "                   | 'flights:read' >> beam.io.ReadFromBigQuery(\n",
    "                       query=sample_query,\n",
    "                       use_standard_sql=True\n",
    "                   )\n",
    "                   | 'flights:tzcorr' >> beam.FlatMap(\n",
    "                       tz_correct, beam.pvalue.AsDict(airports)\n",
    "                   )\n",
    "                   )\n",
    "\n",
    "        (flights\n",
    "         | 'flights:tostring' >> beam.Map(lambda fields: json.dumps(fields))\n",
    "         | 'flights:out' >> beam.io.textio.WriteToText(flights_output)\n",
    "         )\n",
    "\n",
    "\n",
    "        events = flights | beam.FlatMap(get_next_event)\n",
    "\n",
    "        (events\n",
    "         | 'events:tostring' >> beam.Map(lambda fields: json.dumps(fields))\n",
    "         | 'events:out' >> beam.io.textio.WriteToText('df06_all_events')\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:apache_beam.runners.common:ReadFromBigQuery requires a GCS location to be provided. Neither gcs_location in the constructor nor the fallback option --temp_location is set. [while running 'flights:read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction']\n",
      "Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 1435, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 636, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 1611, in apache_beam.runners.common._OutputHandler.handle_process_outputs\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1576, in process\n",
      "    for part, size in self.restriction_provider.split_and_size(\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/transforms/core.py\", line 333, in split_and_size\n",
      "    for part in self.split(element, restriction):\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/iobase.py\", line 1652, in split\n",
      "    for source_bundle in source_bundles:\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py\", line 813, in split\n",
      "    schema, metadata_list = self._export_files(bq)\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py\", line 884, in _export_files\n",
      "    gcs_location = bigquery_export_destination_uri(\n",
      "  File \"/home/inspired/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery_read_internal.py\", line 89, in bigquery_export_destination_uri\n",
      "    raise ValueError(\n",
      "ValueError: ReadFromBigQuery requires a GCS location to be provided. Neither gcs_location in the constructor nor the fallback option --temp_location is set.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ReadFromBigQuery requires a GCS location to be provided. Neither gcs_location in the constructor nor the fallback option --temp_location is set. [while running 'flights:read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1435\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:636\u001b[0m, in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1611\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler.handle_process_outputs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py:1576\u001b[0m, in \u001b[0;36mcreate_split_and_size_restrictions.<locals>.SplitAndSizeRestrictions.process\u001b[0;34m(self, element_restriction, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m element, (restriction, _) \u001b[38;5;241m=\u001b[39m element_restriction\n\u001b[0;32m-> 1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestriction_provider\u001b[38;5;241m.\u001b[39msplit_and_size(\n\u001b[1;32m   1577\u001b[0m     element, restriction):\n\u001b[1;32m   1578\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/transforms/core.py:333\u001b[0m, in \u001b[0;36mRestrictionProvider.split_and_size\u001b[0;34m(self, element, restriction)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like split, but also does sizing, returning (restriction, size) pairs.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mFor each pair, size must be non-negative.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mimplemented.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit(element, restriction):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m part, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestriction_size(element, part)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/iobase.py:1652\u001b[0m, in \u001b[0;36m_SDFBoundedSourceRestrictionProvider.split\u001b[0;34m(self, element, restriction)\u001b[0m\n\u001b[1;32m   1651\u001b[0m source_bundles \u001b[38;5;241m=\u001b[39m restriction\u001b[38;5;241m.\u001b[39msource()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_desired_chunk_size)\n\u001b[0;32m-> 1652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_bundle \u001b[38;5;129;01min\u001b[39;00m source_bundles:\n\u001b[1;32m   1653\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m _SDFBoundedSourceRestriction(source_bundle)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:813\u001b[0m, in \u001b[0;36m_CustomBigQuerySource.split\u001b[0;34m(self, desired_bundle_size, start_position, stop_position)\u001b[0m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_reference\u001b[38;5;241m.\u001b[39mprojectId \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_project()\n\u001b[0;32m--> 813\u001b[0m schema, metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_result \u001b[38;5;241m=\u001b[39m _BigQueryExportResult(\n\u001b[1;32m    815\u001b[0m     coder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoder(schema),\n\u001b[1;32m    816\u001b[0m     paths\u001b[38;5;241m=\u001b[39m[metadata\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m metadata_list])\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:884\u001b[0m, in \u001b[0;36m_CustomBigQuerySource._export_files\u001b[0;34m(self, bq)\u001b[0m\n\u001b[1;32m    883\u001b[0m temp_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mview_as(GoogleCloudOptions)\u001b[38;5;241m.\u001b[39mtemp_location\n\u001b[0;32m--> 884\u001b[0m gcs_location \u001b[38;5;241m=\u001b[39m \u001b[43mbigquery_export_destination_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcs_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_uuid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery_read_internal.py:89\u001b[0m, in \u001b[0;36mbigquery_export_destination_uri\u001b[0;34m(gcs_location_vp, temp_location, unique_id, directory_only)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     90\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReadFromBigQuery requires a GCS location to be provided. Neither \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     91\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgcs_location in the constructor nor the fallback option \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--temp_location is set.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unique_id:\n",
      "\u001b[0;31mValueError\u001b[0m: ReadFromBigQuery requires a GCS location to be provided. Neither gcs_location in the constructor nor the fallback option --temp_location is set.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigquery-manu-407202\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(project)\u001b[0m\n\u001b[1;32m     15\u001b[0m sample_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.dsongcp.flights_sample\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m flights_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf06_all_flights\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(argv\u001b[38;5;241m=\u001b[39margv) \u001b[38;5;28;01mas\u001b[39;00m pipeline:\n\u001b[1;32m     19\u001b[0m     airports \u001b[38;5;241m=\u001b[39m (pipeline\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairports:read\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromBigQuery(\n\u001b[1;32m     21\u001b[0m                     query\u001b[38;5;241m=\u001b[39mairports_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 )\n\u001b[1;32m     33\u001b[0m                 )\n\u001b[1;32m     35\u001b[0m     flights \u001b[38;5;241m=\u001b[39m (pipeline\n\u001b[1;32m     36\u001b[0m                \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflights:read\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromBigQuery(\n\u001b[1;32m     37\u001b[0m                    query\u001b[38;5;241m=\u001b[39msample_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                )\n\u001b[1;32m     43\u001b[0m                )\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/pipeline.py:612\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exc_type:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mwait_until_finish()\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/pipeline.py:586\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m       shutil\u001b[38;5;241m.\u001b[39mrmtree(tmpdir)\n\u001b[0;32m--> 586\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_in_ipython():\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/direct/direct_runner.py:128\u001b[0m, in \u001b[0;36mSwitchingDirectRunner.run_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m   runner \u001b[38;5;241m=\u001b[39m BundleBasedDirectRunner()\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:202\u001b[0m, in \u001b[0;36mFnApiRunner.run_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    191\u001b[0m   _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    192\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf direct_num_workers is not equal to 1, direct_running_mode \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    193\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould be `multi_processing` or `multi_threading` instead of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers,\n\u001b[1;32m    197\u001b[0m       running_mode)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profiler_factory \u001b[38;5;241m=\u001b[39m Profile\u001b[38;5;241m.\u001b[39mfactory_from_options(\n\u001b[1;32m    200\u001b[0m     options\u001b[38;5;241m.\u001b[39mview_as(pipeline_options\u001b[38;5;241m.\u001b[39mProfilingOptions))\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_latest_run_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_via_runner_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_runner_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_environment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_environment\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_latest_run_result\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:224\u001b[0m, in \u001b[0;36mFnApiRunner.run_via_runner_api\u001b[0;34m(self, pipeline_proto, options)\u001b[0m\n\u001b[1;32m    222\u001b[0m   pipeline_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_default_docker_image(pipeline_proto)\n\u001b[1;32m    223\u001b[0m stage_context, stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_stages(pipeline_proto)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:466\u001b[0m, in \u001b[0;36mFnApiRunner.run_stages\u001b[0;34m(self, stage_context, stages)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m consuming_stage_name \u001b[38;5;241m==\u001b[39m bundle_context_manager\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    465\u001b[0m bundle_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 466\u001b[0m bundle_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_bundle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunner_execution_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbundle_context_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbundle_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consuming_stage_name \u001b[38;5;129;01min\u001b[39;00m monitoring_infos_by_stage:\n\u001b[1;32m    470\u001b[0m   monitoring_infos_by_stage[\n\u001b[1;32m    471\u001b[0m       consuming_stage_name] \u001b[38;5;241m=\u001b[39m consolidate_monitoring_infos(\n\u001b[1;32m    472\u001b[0m           itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    473\u001b[0m               bundle_results\u001b[38;5;241m.\u001b[39mprocess_bundle\u001b[38;5;241m.\u001b[39mmonitoring_infos,\n\u001b[1;32m    474\u001b[0m               monitoring_infos_by_stage[consuming_stage_name]))\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:794\u001b[0m, in \u001b[0;36mFnApiRunner._execute_bundle\u001b[0;34m(self, runner_execution_context, bundle_context_manager, bundle_input)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# We create the bundle manager here, as it can be reused for bundles of\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# the same stage, but it may have to be created by-bundle later on.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m bundle_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bundle_manager(bundle_context_manager)\n\u001b[1;32m    793\u001b[0m last_result, deferred_inputs, newly_set_timers, watermark_updates \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_bundle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrunner_execution_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbundle_context_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbundle_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbundle_context_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_data_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbundle_context_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_timer_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbundle_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pc_name, watermark \u001b[38;5;129;01min\u001b[39;00m watermark_updates\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    803\u001b[0m   _BUNDLE_LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, pc_name, watermark)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:1031\u001b[0m, in \u001b[0;36mFnApiRunner._run_bundle\u001b[0;34m(self, runner_execution_context, bundle_context_manager, bundle_input, data_output, expected_timer_output, bundle_manager)\u001b[0m\n\u001b[1;32m   1022\u001b[0m input_timers \u001b[38;5;241m=\u001b[39m bundle_input\u001b[38;5;241m.\u001b[39mtimers\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_bundle_multiple_times_for_testing(\n\u001b[1;32m   1024\u001b[0m     runner_execution_context,\n\u001b[1;32m   1025\u001b[0m     bundle_manager,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     input_timers,\n\u001b[1;32m   1029\u001b[0m     expected_timer_output)\n\u001b[0;32m-> 1031\u001b[0m result, splits \u001b[38;5;241m=\u001b[39m \u001b[43mbundle_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_bundle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_timers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_timer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# Now we collect all the deferred inputs remaining from bundle execution.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# Deferred inputs can be:\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# - timers\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# - SDK-initiated deferred applications of root elements\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m# - Runner-initiated deferred applications of root elements\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m deferred_inputs \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# type: Dict[str, execution.PartitionableBuffer]\u001b[39;00m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py:1367\u001b[0m, in \u001b[0;36mBundleManager.process_bundle\u001b[0;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# Actually start the bundle.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m process_bundle_req \u001b[38;5;241m=\u001b[39m beam_fn_api_pb2\u001b[38;5;241m.\u001b[39mInstructionRequest(\n\u001b[1;32m   1362\u001b[0m     instruction_id\u001b[38;5;241m=\u001b[39mprocess_bundle_id,\n\u001b[1;32m   1363\u001b[0m     process_bundle\u001b[38;5;241m=\u001b[39mbeam_fn_api_pb2\u001b[38;5;241m.\u001b[39mProcessBundleRequest(\n\u001b[1;32m   1364\u001b[0m         process_bundle_descriptor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbundle_context_manager\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m   1365\u001b[0m         process_bundle_descriptor\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m   1366\u001b[0m         cache_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_token_generator)]))\n\u001b[0;32m-> 1367\u001b[0m result_future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_bundle_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m split_results \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressRequester(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_handler,\n\u001b[1;32m   1371\u001b[0m                        process_bundle_id,\n\u001b[1;32m   1372\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_frequency):\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py:384\u001b[0m, in \u001b[0;36mEmbeddedWorkerHandler.push\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    382\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uid_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    383\u001b[0m   request\u001b[38;5;241m.\u001b[39minstruction_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uid_counter\n\u001b[0;32m--> 384\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ControlFuture(request\u001b[38;5;241m.\u001b[39minstruction_id, response)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py:650\u001b[0m, in \u001b[0;36mSdkWorker.do_instruction\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    647\u001b[0m request_type \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mWhichOneof(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_type:\n\u001b[1;32m    649\u001b[0m   \u001b[38;5;66;03m# E.g. if register is set, this will call self.register(request.register))\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstruction_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    653\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py:688\u001b[0m, in \u001b[0;36mSdkWorker.process_bundle\u001b[0;34m(self, request, instruction_id)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bundle_processor\u001b[38;5;241m.\u001b[39mstate_handler\u001b[38;5;241m.\u001b[39mprocess_instruction_id(\n\u001b[1;32m    685\u001b[0m     instruction_id, request\u001b[38;5;241m.\u001b[39mcache_tokens):\n\u001b[1;32m    686\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_profile(instruction_id):\n\u001b[1;32m    687\u001b[0m     delayed_applications, requests_finalization \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 688\u001b[0m         \u001b[43mbundle_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_bundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    689\u001b[0m     monitoring_infos \u001b[38;5;241m=\u001b[39m bundle_processor\u001b[38;5;241m.\u001b[39mmonitoring_infos()\n\u001b[1;32m    690\u001b[0m     response \u001b[38;5;241m=\u001b[39m beam_fn_api_pb2\u001b[38;5;241m.\u001b[39mInstructionResponse(\n\u001b[1;32m    691\u001b[0m         instruction_id\u001b[38;5;241m=\u001b[39minstruction_id,\n\u001b[1;32m    692\u001b[0m         process_bundle\u001b[38;5;241m=\u001b[39mbeam_fn_api_pb2\u001b[38;5;241m.\u001b[39mProcessBundleResponse(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m             },\n\u001b[1;32m    699\u001b[0m             requires_finalization\u001b[38;5;241m=\u001b[39mrequests_finalization))\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py:1113\u001b[0m, in \u001b[0;36mBundleProcessor.process_bundle\u001b[0;34m(self, instruction_id)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops[element\u001b[38;5;241m.\u001b[39mtransform_id]\u001b[38;5;241m.\u001b[39mprocess_timer(\n\u001b[1;32m   1111\u001b[0m             element\u001b[38;5;241m.\u001b[39mtimer_family_id, timer_data)\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element, beam_fn_api_pb2\u001b[38;5;241m.\u001b[39mElements\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m-> 1113\u001b[0m       \u001b[43minput_op_by_transform_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_encoded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m          \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# Finish all operations.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py:237\u001b[0m, in \u001b[0;36mDataInputOperation.process_encoded\u001b[0;34m(self, encoded_windowed_values)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exn:\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError decoding input stream with coder \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    236\u001b[0m       \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindowed_coder)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexn\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:570\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:572\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:263\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:266\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:953\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:954\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1437\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1526\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1435\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:636\u001b[0m, in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1621\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler.handle_process_outputs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1734\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler._write_value_to_tag\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:266\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:953\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:954\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1437\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1526\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1435\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:636\u001b[0m, in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1621\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler.handle_process_outputs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1734\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler._write_value_to_tag\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:266\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:953\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py:954\u001b[0m, in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1437\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1547\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1435\u001b[0m, in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:636\u001b[0m, in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/common.py:1611\u001b[0m, in \u001b[0;36mapache_beam.runners.common._OutputHandler.handle_process_outputs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py:1576\u001b[0m, in \u001b[0;36mcreate_split_and_size_restrictions.<locals>.SplitAndSizeRestrictions.process\u001b[0;34m(self, element_restriction, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, element_restriction, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1575\u001b[0m   element, (restriction, _) \u001b[38;5;241m=\u001b[39m element_restriction\n\u001b[0;32m-> 1576\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m part, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestriction_provider\u001b[38;5;241m.\u001b[39msplit_and_size(\n\u001b[1;32m   1577\u001b[0m       element, restriction):\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1579\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected size >= 0 but received \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m size)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/transforms/core.py:333\u001b[0m, in \u001b[0;36mRestrictionProvider.split_and_size\u001b[0;34m(self, element, restriction)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_and_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, restriction):\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Like split, but also does sizing, returning (restriction, size) pairs.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m  For each pair, size must be non-negative.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  implemented.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit(element, restriction):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m part, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestriction_size(element, part)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/iobase.py:1652\u001b[0m, in \u001b[0;36m_SDFBoundedSourceRestrictionProvider.split\u001b[0;34m(self, element, restriction)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;66;03m# Invoke source.split to get initial splitting results.\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m source_bundles \u001b[38;5;241m=\u001b[39m restriction\u001b[38;5;241m.\u001b[39msource()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_desired_chunk_size)\n\u001b[0;32m-> 1652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_bundle \u001b[38;5;129;01min\u001b[39;00m source_bundles:\n\u001b[1;32m   1653\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m _SDFBoundedSourceRestriction(source_bundle)\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:813\u001b[0m, in \u001b[0;36m_CustomBigQuerySource.split\u001b[0;34m(self, desired_bundle_size, start_position, stop_position)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_reference\u001b[38;5;241m.\u001b[39mprojectId:\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable_reference\u001b[38;5;241m.\u001b[39mprojectId \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_project()\n\u001b[0;32m--> 813\u001b[0m schema, metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_result \u001b[38;5;241m=\u001b[39m _BigQueryExportResult(\n\u001b[1;32m    815\u001b[0m     coder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoder(schema),\n\u001b[1;32m    816\u001b[0m     paths\u001b[38;5;241m=\u001b[39m[metadata\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m metadata_list])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:884\u001b[0m, in \u001b[0;36m_CustomBigQuerySource._export_files\u001b[0;34m(self, bq)\u001b[0m\n\u001b[1;32m    878\u001b[0m export_job_name \u001b[38;5;241m=\u001b[39m bigquery_tools\u001b[38;5;241m.\u001b[39mgenerate_bq_job_name(\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job_name,\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source_uuid,\n\u001b[1;32m    881\u001b[0m     bigquery_tools\u001b[38;5;241m.\u001b[39mBigQueryJobTypes\u001b[38;5;241m.\u001b[39mEXPORT,\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()), random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)))\n\u001b[1;32m    883\u001b[0m temp_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mview_as(GoogleCloudOptions)\u001b[38;5;241m.\u001b[39mtemp_location\n\u001b[0;32m--> 884\u001b[0m gcs_location \u001b[38;5;241m=\u001b[39m \u001b[43mbigquery_export_destination_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcs_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_uuid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_json_exports:\n",
      "File \u001b[0;32m~/data-science-on-gcp/04_streaming/.beam-04/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery_read_internal.py:89\u001b[0m, in \u001b[0;36mbigquery_export_destination_uri\u001b[0;34m(gcs_location_vp, temp_location, unique_id, directory_only)\u001b[0m\n\u001b[1;32m     87\u001b[0m   _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcs_location is empty, using temp_location instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     90\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReadFromBigQuery requires a GCS location to be provided. Neither \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     91\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgcs_location in the constructor nor the fallback option \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--temp_location is set.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unique_id:\n\u001b[1;32m     95\u001b[0m   unique_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "\u001b[0;31mValueError\u001b[0m: ReadFromBigQuery requires a GCS location to be provided. Neither gcs_location in the constructor nor the fallback option --temp_location is set. [while running 'flights:read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction']"
     ]
    }
   ],
   "source": [
    "pr = 'bigquery-manu-407202'\n",
    "run(pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".beam-04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
