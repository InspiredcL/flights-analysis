{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing, Training, and Deploying a TensorFlow model on Google Cloud Platform (completely within Jupyter)\n",
    "\n",
    "\n",
    "In Chapter 9 of [Data Science on the Google Cloud Platform](http://shop.oreilly.com/product/0636920057628.do), I trained a TensorFlow Estimator model to predict flight delays.\n",
    "\n",
    "In this notebook, we'll modernize the workflow:\n",
    "* Use eager mode for TensorFlow development\n",
    "* Use tf.data to write the input pipeline\n",
    "* Run the notebook as-is on Cloud using Deep Learning VM or Kubeflow pipelines\n",
    "* Deploy the trained model to AI Platform as a web service\n",
    "\n",
    "The combination of eager mode, tf.data and DLVM/KFP makes this workflow a lot easier.\n",
    "We don't need to deal with Python packages or Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "# In \"production\", these will be replaced by the parameters passed to papermill\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'\n",
    "DEVELOP_MODE = True\n",
    "NBUCKETS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET = \"gs://cloud-training-demos/flights/chapter8/output/\"\n",
    "TRAIN_DATA_PATTERN = DATA_BUCKET + \"train*\"\n",
    "VALID_DATA_PATTERN = DATA_BUCKET + \"test*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos/flights/chapter8/output/delays.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00006-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00006-of-00007.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def load_dataset(pattern):\n",
    "  return tf.data.experimental.make_csv_dataset(pattern, 1, CSV_COLUMNS, DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arr_lon': array([-90.075836], dtype=float32), 'dest': array([b'JAN'], dtype=object), 'dep_delay': array([-1.], dtype=float32), 'arr_lat': array([32.31111], dtype=float32), 'taxiout': array([20.], dtype=float32), 'dep_lat': array([33.636665], dtype=float32), 'avg_dep_delay': array([33.790806], dtype=float32), 'carrier': array([b'EV'], dtype=object), 'ontime': array([1.], dtype=float32), 'dep_lon': array([-84.42778], dtype=float32), 'origin': array([b'ATL'], dtype=object), 'avg_arr_delay': array([-4.], dtype=float32), 'distance': array([341.], dtype=float32)}\n",
      "{'arr_lon': array([-90.075836], dtype=float32), 'dest': array([b'JAN'], dtype=object), 'dep_delay': array([-3.], dtype=float32), 'arr_lat': array([32.31111], dtype=float32), 'taxiout': array([19.], dtype=float32), 'dep_lat': array([32.896946], dtype=float32), 'avg_dep_delay': array([27.40805], dtype=float32), 'carrier': array([b'MQ'], dtype=object), 'ontime': array([1.], dtype=float32), 'dep_lon': array([-97.038055], dtype=float32), 'origin': array([b'DFW'], dtype=object), 'avg_arr_delay': array([3.5], dtype=float32), 'distance': array([408.], dtype=float32)}\n",
      "{'arr_lon': array([-90.075836], dtype=float32), 'dest': array([b'JAN'], dtype=object), 'dep_delay': array([-7.], dtype=float32), 'arr_lat': array([32.31111], dtype=float32), 'taxiout': array([28.], dtype=float32), 'dep_lat': array([33.636665], dtype=float32), 'avg_dep_delay': array([26.38645], dtype=float32), 'carrier': array([b'DL'], dtype=object), 'ontime': array([1.], dtype=float32), 'dep_lon': array([-84.42778], dtype=float32), 'origin': array([b'ATL'], dtype=object), 'avg_arr_delay': array([114.], dtype=float32), 'distance': array([341.], dtype=float32)}\n",
      "{'arr_lon': array([-80.04056], dtype=float32), 'dest': array([b'CHS'], dtype=object), 'dep_delay': array([14.], dtype=float32), 'arr_lat': array([32.898613], dtype=float32), 'taxiout': array([10.], dtype=float32), 'dep_lat': array([39.175278], dtype=float32), 'avg_dep_delay': array([24.121355], dtype=float32), 'carrier': array([b'WN'], dtype=object), 'ontime': array([1.], dtype=float32), 'dep_lon': array([-76.668335], dtype=float32), 'origin': array([b'BWI'], dtype=object), 'avg_arr_delay': array([13.], dtype=float32), 'distance': array([472.], dtype=float32)}\n",
      "{'arr_lon': array([-90.075836], dtype=float32), 'dest': array([b'JAN'], dtype=object), 'dep_delay': array([24.], dtype=float32), 'arr_lat': array([32.31111], dtype=float32), 'taxiout': array([12.], dtype=float32), 'dep_lat': array([41.979443], dtype=float32), 'avg_dep_delay': array([34.119404], dtype=float32), 'carrier': array([b'EV'], dtype=object), 'ontime': array([1.], dtype=float32), 'dep_lon': array([-87.9075], dtype=float32), 'origin': array([b'ORD'], dtype=object), 'avg_arr_delay': array([1.], dtype=float32), 'distance': array([677.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "if DEVELOP_MODE:\n",
    "    dataset = load_dataset(TRAIN_DATA_PATTERN)\n",
    "    for n, data in enumerate(dataset):\n",
    "        numpy_data = {k: v.numpy() for k, v in data.items()} # .numpy() works only in eager mode\n",
    "        print(numpy_data)\n",
    "        if n>3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example_input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile example_input.json\n",
    "{\"dep_delay\": 14.0, \"taxiout\": 13.0, \"distance\": 319.0, \"avg_dep_delay\": 25.863039, \"avg_arr_delay\": 27.0, \"carrier\": \"WN\", \"dep_lat\": 32.84722, \"dep_lon\": -96.85167, \"arr_lat\": 31.9425, \"arr_lon\": -102.20194, \"origin\": \"DAL\", \"dest\": \"MAF\"}\n",
    "{\"dep_delay\": -9.0, \"taxiout\": 21.0, \"distance\": 301.0, \"avg_dep_delay\": 41.050808, \"avg_arr_delay\": -7.0, \"carrier\": \"EV\", \"dep_lat\": 29.984444, \"dep_lon\": -95.34139, \"arr_lat\": 27.544167, \"arr_lon\": -99.46167, \"origin\": \"IAH\", \"dest\": \"LRD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling prepare\n",
      "[(OrderedDict([('dep_delay', <tf.Tensor: id=345, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[ 0.],\n",
      "       [-1.],\n",
      "       [-5.],\n",
      "       [-6.],\n",
      "       [-3.]], dtype=float32)>), ('taxiout', <tf.Tensor: id=351, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[20.],\n",
      "       [50.],\n",
      "       [20.],\n",
      "       [17.],\n",
      "       [14.]], dtype=float32)>), ('distance', <tf.Tensor: id=349, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[145.],\n",
      "       [119.],\n",
      "       [119.],\n",
      "       [145.],\n",
      "       [106.]], dtype=float32)>), ('avg_dep_delay', <tf.Tensor: id=343, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[26.245787],\n",
      "       [26.036997],\n",
      "       [24.971495],\n",
      "       [24.760445],\n",
      "       [25.268755]], dtype=float32)>), ('avg_arr_delay', <tf.Tensor: id=342, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[  0. ],\n",
      "       [ 15. ],\n",
      "       [  9.5],\n",
      "       [ -9. ],\n",
      "       [-16. ]], dtype=float32)>), ('carrier', <tf.Tensor: id=344, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'EV'],\n",
      "       [b'OO'],\n",
      "       [b'OO'],\n",
      "       [b'EV'],\n",
      "       [b'EV']], dtype=object)>), ('dep_lat', <tf.Tensor: id=346, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[33.636665],\n",
      "       [44.881943],\n",
      "       [44.881943],\n",
      "       [33.636665],\n",
      "       [33.636665]], dtype=float32)>), ('dep_lon', <tf.Tensor: id=347, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-84.42778 ],\n",
      "       [-93.221664],\n",
      "       [-93.221664],\n",
      "       [-84.42778 ],\n",
      "       [-84.42778 ]], dtype=float32)>), ('arr_lat', <tf.Tensor: id=340, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[31.535555],\n",
      "       [43.879166],\n",
      "       [43.879166],\n",
      "       [31.535555],\n",
      "       [35.03528 ]], dtype=float32)>), ('arr_lon', <tf.Tensor: id=341, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-84.19444],\n",
      "       [-91.25667],\n",
      "       [-91.25667],\n",
      "       [-84.19444],\n",
      "       [-85.20361]], dtype=float32)>), ('origin', <tf.Tensor: id=350, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'ATL'],\n",
      "       [b'MSP'],\n",
      "       [b'MSP'],\n",
      "       [b'ATL'],\n",
      "       [b'ATL']], dtype=object)>), ('dest', <tf.Tensor: id=348, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'ABY'],\n",
      "       [b'LSE'],\n",
      "       [b'LSE'],\n",
      "       [b'ABY'],\n",
      "       [b'CHA']], dtype=object)>)]), <tf.Tensor: id=352, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "def features_and_labels(features):\n",
    "  label = features.pop('ontime') # this is what we will train for\n",
    "  return features, label\n",
    "\n",
    "def prepare_dataset(pattern, batch_size, truncate=None, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "  dataset = load_dataset(pattern)\n",
    "  dataset = dataset.map(features_and_labels)\n",
    "  dataset = dataset.cache()\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(1)\n",
    "  if truncate is not None:\n",
    "    dataset = dataset.take(truncate)\n",
    "  return dataset\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(\"Calling prepare\")\n",
    "    one_item = prepare_dataset(TRAIN_DATA_PATTERN, batch_size=5, truncate=1)\n",
    "    print(list(one_item)) # should print one batch of 2 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow wide-and-deep model\n",
    "\n",
    "We'll create feature columns, and do some discretization and feature engineering.\n",
    "See the book for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.feature_column as fc\n",
    "\n",
    "real = {\n",
    "    colname : fc.numeric_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "}\n",
    "sparse = {\n",
    "      'carrier': fc.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "      'origin' : fc.categorical_column_with_hash_bucket('origin', hash_bucket_size=1000),\n",
    "      'dest'   : fc.categorical_column_with_hash_bucket('dest', hash_bucket_size=1000)\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in real.keys()\n",
    "}\n",
    "inputs.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='string') \\\n",
    "          for colname in sparse.keys()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['carrier', 'dest', 'origin', 'dep_loc', 'arr_loc', 'ori_dest', 'dep_arr'])\n",
      "dict_keys(['arr_lon', 'dep_delay', 'arr_lat', 'taxiout', 'embed_dep_arr', 'dep_lat', 'embed_dep_loc', 'embed_origin', 'embed_ori_dest', 'embed_carrier', 'embed_dest', 'distance', 'avg_dep_delay', 'dep_lon', 'embed_arr_loc', 'avg_arr_delay'])\n"
     ]
    }
   ],
   "source": [
    "latbuckets = np.linspace(20.0, 50.0, NBUCKETS).tolist()  # USA\n",
    "lonbuckets = np.linspace(-120.0, -70.0, NBUCKETS).tolist() # USA\n",
    "disc = {}\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "})\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "})\n",
    "\n",
    "# cross columns that make sense in combination\n",
    "sparse['dep_loc'] = fc.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['arr_loc'] = fc.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['dep_arr'] = fc.crossed_column([sparse['dep_loc'], sparse['arr_loc']], NBUCKETS ** 4)\n",
    "sparse['ori_dest'] = fc.crossed_column(['origin', 'dest'], hash_bucket_size=1000)\n",
    "\n",
    "# embed all the sparse columns\n",
    "embed = {\n",
    "       'embed_{}'.format(colname) : fc.embedding_column(col, 10) \\\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "real.update(embed)\n",
    "\n",
    "# one-hot encode the sparse columns\n",
    "sparse = {\n",
    "    colname : fc.indicator_column(col) \\\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(sparse.keys())\n",
    "    print(real.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and evaluate once in a while\n",
    "\n",
    "Also checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trained model to gs://cloud-training-demos-ml/flights/trained_model\n"
     ]
    }
   ],
   "source": [
    "model_dir='gs://{}/flights/trained_model'.format(BUCKET)\n",
    "os.environ['OUTDIR'] = model_dir  # needed for deployment\n",
    "print('Writing trained model to {}'.format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -rf $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "arr_lat (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arr_lon (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "avg_arr_delay (InputLayer)      [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "avg_dep_delay (InputLayer)      [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "carrier (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dep_delay (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dep_lat (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dep_lon (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dest (InputLayer)               [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distance (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "origin (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "taxiout (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_2 (DenseFeatures (None, 79)           36890       arr_lat[0][0]                    \n",
      "                                                                 arr_lon[0][0]                    \n",
      "                                                                 avg_arr_delay[0][0]              \n",
      "                                                                 avg_dep_delay[0][0]              \n",
      "                                                                 carrier[0][0]                    \n",
      "                                                                 dep_delay[0][0]                  \n",
      "                                                                 dep_lat[0][0]                    \n",
      "                                                                 dep_lon[0][0]                    \n",
      "                                                                 dest[0][0]                       \n",
      "                                                                 distance[0][0]                   \n",
      "                                                                 origin[0][0]                     \n",
      "                                                                 taxiout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           5120        dense_features_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_3 (DenseFeatures (None, 3689)         0           arr_lat[0][0]                    \n",
      "                                                                 arr_lon[0][0]                    \n",
      "                                                                 avg_arr_delay[0][0]              \n",
      "                                                                 avg_dep_delay[0][0]              \n",
      "                                                                 carrier[0][0]                    \n",
      "                                                                 dep_delay[0][0]                  \n",
      "                                                                 dep_lat[0][0]                    \n",
      "                                                                 dep_lon[0][0]                    \n",
      "                                                                 dest[0][0]                       \n",
      "                                                                 distance[0][0]                   \n",
      "                                                                 origin[0][0]                     \n",
      "                                                                 taxiout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3721)         0           dense_4[0][0]                    \n",
      "                                                                 dense_features_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            3722        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 47,812\n",
      "Trainable params: 47,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a wide-and-deep model.\n",
    "def wide_and_deep_classifier(inputs, linear_feature_columns, dnn_feature_columns, dnn_hidden_units):\n",
    "    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns)(inputs)\n",
    "    for numnodes in dnn_hidden_units:\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu')(deep)        \n",
    "    wide = tf.keras.layers.DenseFeatures(linear_feature_columns)(inputs)\n",
    "    both = tf.keras.layers.concatenate([deep, wide])\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(both)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = wide_and_deep_classifier(\n",
    "    inputs,\n",
    "    linear_feature_columns = sparse.values(),\n",
    "    dnn_feature_columns = real.values(),\n",
    "    dnn_hidden_units = [64, 32])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 65s 65s/step - loss: 1.9135 - accuracy: 0.1875 - val_loss: 1.5502 - val_accuracy: 0.1546\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 65s 65s/step - loss: 0.9439 - accuracy: 0.2344 - val_loss: 0.5983 - val_accuracy: 0.7562\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.5457 - accuracy: 0.8438 - val_loss: 0.4300 - val_accuracy: 0.8526\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.3936 - accuracy: 0.8594 - val_loss: 0.5586 - val_accuracy: 0.8514\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.5546 - accuracy: 0.7969 - val_loss: 0.6831 - val_accuracy: 0.8512\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 65s 65s/step - loss: 0.5631 - accuracy: 0.8594 - val_loss: 0.7669 - val_accuracy: 0.8511\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.2470 - accuracy: 0.9375 - val_loss: 0.8354 - val_accuracy: 0.8510\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.8415 - accuracy: 0.7812 - val_loss: 0.8674 - val_accuracy: 0.8511\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.8577 - accuracy: 0.8125 - val_loss: 0.8718 - val_accuracy: 0.8511\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.7416 - accuracy: 0.8281 - val_loss: 0.8539 - val_accuracy: 0.8512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79603af8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and evaluation dataset\n",
    "train_batch_size = 64\n",
    "eval_batch_size = 100 if DEVELOP_MODE else 10000\n",
    "num_steps = 10 if DEVELOP_MODE else (1000000 // train_batch_size)\n",
    "train_dataset = prepare_dataset(TRAIN_DATA_PATTERN, train_batch_size)\n",
    "eval_dataset = prepare_dataset(VALID_DATA_PATTERN, eval_batch_size, eval_batch_size*10, tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          validation_data=eval_dataset,\n",
    "          epochs=num_steps, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/flights/trained_model/export/exporter/1553294283/\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['arr_lat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_7:0\n",
      "    inputs['arr_lon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_8:0\n",
      "    inputs['avg_arr_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_4:0\n",
      "    inputs['avg_dep_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_3:0\n",
      "    inputs['carrier'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_9:0\n",
      "    inputs['dep_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder:0\n",
      "    inputs['dep_lat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_5:0\n",
      "    inputs['dep_lon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_6:0\n",
      "    inputs['dest'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_11:0\n",
      "    inputs['distance'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_2:0\n",
      "    inputs['origin'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_10:0\n",
      "    inputs['taxiout'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/ExpandDims:0\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/str_classes:0\n",
      "    outputs['logistic'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/logistic:0\n",
      "    outputs['logits'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: add:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 2)\n",
      "        name: head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo $model_dir\n",
    "saved_model_cli show --dir ${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"flights\"\n",
    "MODEL_VERSION=\"kfp\"\n",
    "TFVERSION=\"2.0\"\n",
    "MODEL_LOCATION=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#yes | gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine predict --model=flights --version=kfp --json-instances=example_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
