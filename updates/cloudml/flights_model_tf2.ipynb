{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing, Training, and Deploying a TensorFlow model on Google Cloud Platform (completely within Jupyter)\n",
    "\n",
    "\n",
    "In Chapter 9 of [Data Science on the Google Cloud Platform](http://shop.oreilly.com/product/0636920057628.do), I trained a TensorFlow Estimator model to predict flight delays.\n",
    "\n",
    "In this notebook, we'll modernize the workflow:\n",
    "* Use eager mode for TensorFlow development\n",
    "* Use tf.data to write the input pipeline\n",
    "* Run the notebook as-is on Cloud using Deep Learning VM or Kubeflow pipelines\n",
    "* Deploy the trained model to Cloud ML Engine as a web service\n",
    "\n",
    "The combination of eager mode, tf.data and DLVM/KFP makes this workflow a lot easier.\n",
    "We don't need to deal with Python packages or Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "# In \"production\", these will be replaced by the parameters passed to papermill\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'\n",
    "DEVELOP_MODE = True\n",
    "NBUCKETS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET = \"gs://cloud-training-demos/flights/chapter8/output/\"\n",
    "TRAIN_DATA_PATTERN = DATA_BUCKET + \"train*\"\n",
    "VALID_DATA_PATTERN = DATA_BUCKET + \"test*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos/flights/chapter8/output/delays.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00006-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00006-of-00007.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def load_dataset(pattern):\n",
    "  return tf.data.experimental.make_csv_dataset(pattern, 1, CSV_COLUMNS, DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-3.], dtype=float32), 'taxiout': array([14.], dtype=float32), 'distance': array([824.], dtype=float32), 'avg_dep_delay': array([26.482693], dtype=float32), 'avg_arr_delay': array([30.], dtype=float32), 'carrier': array([b'NK'], dtype=object), 'dep_lat': array([28.429445], dtype=float32), 'dep_lon': array([-81.30889], dtype=float32), 'arr_lat': array([40.274723], dtype=float32), 'arr_lon': array([-79.40667], dtype=float32), 'origin': array([b'MCO'], dtype=object), 'dest': array([b'LBE'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([3.], dtype=float32), 'taxiout': array([10.], dtype=float32), 'distance': array([733.], dtype=float32), 'avg_dep_delay': array([33.2664], dtype=float32), 'avg_arr_delay': array([19.4], dtype=float32), 'carrier': array([b'AA'], dtype=object), 'dep_lat': array([32.896946], dtype=float32), 'dep_lon': array([-97.038055], dtype=float32), 'arr_lat': array([38.174168], dtype=float32), 'arr_lon': array([-85.73639], dtype=float32), 'origin': array([b'DFW'], dtype=object), 'dest': array([b'SDF'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([11.], dtype=float32), 'taxiout': array([15.], dtype=float32), 'distance': array([287.], dtype=float32), 'avg_dep_delay': array([38.082882], dtype=float32), 'avg_arr_delay': array([-5.5], dtype=float32), 'carrier': array([b'EV'], dtype=object), 'dep_lat': array([41.979443], dtype=float32), 'dep_lon': array([-87.9075], dtype=float32), 'arr_lat': array([38.174168], dtype=float32), 'arr_lon': array([-85.73639], dtype=float32), 'origin': array([b'ORD'], dtype=object), 'dest': array([b'SDF'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-5.], dtype=float32), 'taxiout': array([31.], dtype=float32), 'distance': array([788.], dtype=float32), 'avg_dep_delay': array([25.059471], dtype=float32), 'avg_arr_delay': array([-2.], dtype=float32), 'carrier': array([b'EV'], dtype=object), 'dep_lat': array([29.984444], dtype=float32), 'dep_lon': array([-95.34139], dtype=float32), 'arr_lat': array([38.174168], dtype=float32), 'arr_lon': array([-85.73639], dtype=float32), 'origin': array([b'IAH'], dtype=object), 'dest': array([b'SDF'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-1.], dtype=float32), 'taxiout': array([8.], dtype=float32), 'distance': array([271.], dtype=float32), 'avg_dep_delay': array([27.507399], dtype=float32), 'avg_arr_delay': array([-7.6666665], dtype=float32), 'carrier': array([b'WN'], dtype=object), 'dep_lat': array([41.78583], dtype=float32), 'dep_lon': array([-87.7525], dtype=float32), 'arr_lat': array([38.174168], dtype=float32), 'arr_lon': array([-85.73639], dtype=float32), 'origin': array([b'MDW'], dtype=object), 'dest': array([b'SDF'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "if DEVELOP_MODE:\n",
    "    dataset = load_dataset(TRAIN_DATA_PATTERN)\n",
    "    for n, data in enumerate(dataset):\n",
    "        numpy_data = {k: v.numpy() for k, v in data.items()} # .numpy() works only in eager mode\n",
    "        print(numpy_data)\n",
    "        if n>3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example_input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile example_input.json\n",
    "{\"dep_delay\": 14.0, \"taxiout\": 13.0, \"distance\": 319.0, \"avg_dep_delay\": 25.863039, \"avg_arr_delay\": 27.0, \"carrier\": \"WN\", \"dep_lat\": 32.84722, \"dep_lon\": -96.85167, \"arr_lat\": 31.9425, \"arr_lon\": -102.20194, \"origin\": \"DAL\", \"dest\": \"MAF\"}\n",
    "{\"dep_delay\": -9.0, \"taxiout\": 21.0, \"distance\": 301.0, \"avg_dep_delay\": 41.050808, \"avg_arr_delay\": -7.0, \"carrier\": \"EV\", \"dep_lat\": 29.984444, \"dep_lon\": -95.34139, \"arr_lat\": 27.544167, \"arr_lon\": -99.46167, \"origin\": \"IAH\", \"dest\": \"LRD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling prepare\n",
      "[(OrderedDict([('dep_delay', <tf.Tensor: id=6654, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[26.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [-5.],\n",
      "       [-2.]], dtype=float32)>), ('taxiout', <tf.Tensor: id=6660, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[13.],\n",
      "       [23.],\n",
      "       [13.],\n",
      "       [15.],\n",
      "       [28.]], dtype=float32)>), ('distance', <tf.Tensor: id=6658, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[351.],\n",
      "       [201.],\n",
      "       [201.],\n",
      "       [145.],\n",
      "       [106.]], dtype=float32)>), ('avg_dep_delay', <tf.Tensor: id=6652, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[31.995535],\n",
      "       [22.849628],\n",
      "       [24.786774],\n",
      "       [24.692053],\n",
      "       [26.245787]], dtype=float32)>), ('avg_arr_delay', <tf.Tensor: id=6651, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[  4.],\n",
      "       [  8.],\n",
      "       [ -6.],\n",
      "       [-13.],\n",
      "       [  9.]], dtype=float32)>), ('carrier', <tf.Tensor: id=6653, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'EV'],\n",
      "       [b'EV'],\n",
      "       [b'EV'],\n",
      "       [b'EV'],\n",
      "       [b'EV']], dtype=object)>), ('dep_lat', <tf.Tensor: id=6655, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[32.89722 ],\n",
      "       [29.984444],\n",
      "       [29.984444],\n",
      "       [33.636665],\n",
      "       [33.636665]], dtype=float32)>), ('dep_lon', <tf.Tensor: id=6656, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-97.03778],\n",
      "       [-95.34139],\n",
      "       [-95.34139],\n",
      "       [-84.42778],\n",
      "       [-84.42778]], dtype=float32)>), ('arr_lat', <tf.Tensor: id=6649, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[30.205   ],\n",
      "       [30.205   ],\n",
      "       [30.205   ],\n",
      "       [31.535555],\n",
      "       [35.03528 ]], dtype=float32)>), ('arr_lon', <tf.Tensor: id=6650, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-91.98778],\n",
      "       [-91.98778],\n",
      "       [-91.98778],\n",
      "       [-84.19444],\n",
      "       [-85.20361]], dtype=float32)>), ('origin', <tf.Tensor: id=6659, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'DFW'],\n",
      "       [b'IAH'],\n",
      "       [b'IAH'],\n",
      "       [b'ATL'],\n",
      "       [b'ATL']], dtype=object)>), ('dest', <tf.Tensor: id=6657, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'LFT'],\n",
      "       [b'LFT'],\n",
      "       [b'LFT'],\n",
      "       [b'ABY'],\n",
      "       [b'CHA']], dtype=object)>)]), <tf.Tensor: id=6661, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "def features_and_labels(features):\n",
    "  label = features.pop('ontime') # this is what we will train for\n",
    "  return features, label\n",
    "\n",
    "def prepare_dataset(pattern, batch_size, truncate=None, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "  dataset = load_dataset(pattern)\n",
    "  dataset = dataset.map(features_and_labels)\n",
    "  dataset = dataset.cache()\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(1)\n",
    "  if truncate is not None:\n",
    "    dataset = dataset.take(truncate)\n",
    "  return dataset\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(\"Calling prepare\")\n",
    "    one_item = prepare_dataset(TRAIN_DATA_PATTERN, batch_size=5, truncate=1)\n",
    "    print(list(one_item)) # should print one batch of 2 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow wide-and-deep model\n",
    "\n",
    "We'll create feature columns, and do some discretization and feature engineering.\n",
    "See the book for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.feature_column as fc\n",
    "\n",
    "real = {\n",
    "    colname : fc.numeric_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "}\n",
    "sparse = {\n",
    "      'carrier': fc.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "      'origin' : fc.categorical_column_with_hash_bucket('origin', hash_bucket_size=1000),\n",
    "      'dest'   : fc.categorical_column_with_hash_bucket('dest', hash_bucket_size=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['carrier', 'origin', 'dest', 'dep_loc', 'arr_loc', 'dep_arr', 'ori_dest'])\n",
      "dict_keys(['dep_delay', 'taxiout', 'distance', 'avg_dep_delay', 'avg_arr_delay', 'dep_lat', 'dep_lon', 'arr_lat', 'arr_lon', 'carrier', 'origin', 'dest', 'dep_loc', 'arr_loc', 'dep_arr', 'ori_dest'])\n"
     ]
    }
   ],
   "source": [
    "latbuckets = np.linspace(20.0, 50.0, NBUCKETS).tolist()  # USA\n",
    "lonbuckets = np.linspace(-120.0, -70.0, NBUCKETS).tolist() # USA\n",
    "disc = {}\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "})\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "})\n",
    "\n",
    "# cross columns that make sense in combination\n",
    "sparse['dep_loc'] = fc.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['arr_loc'] = fc.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['dep_arr'] = fc.crossed_column([sparse['dep_loc'], sparse['arr_loc']], NBUCKETS ** 4)\n",
    "sparse['ori_dest'] = fc.crossed_column(['origin', 'dest'], hash_bucket_size=1000)\n",
    "\n",
    "# embed all the sparse columns\n",
    "embed = {\n",
    "       colname : fc.embedding_column(col, 10) \\\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "real.update(embed)\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(sparse.keys())\n",
    "    print(real.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving\n",
    "\n",
    "This serving input function is how the model will be deployed for prediction. We require these fields for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        # All the real-valued columns\n",
    "        column: tf.Variable(dtype=tf.float32, trainable=False, expected_shape=[None], name=column) \\\n",
    "             for column in ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "                            ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    feature_placeholders.update({\n",
    "        column: tf.Variable(dtype=tf.string, trainable=False, expected_shape=[None], name=column) \\\n",
    "             for column in ['carrier', 'origin', 'dest']\n",
    "    })\n",
    "    features = feature_placeholders # no transformations\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and evaluate once in a while\n",
    "\n",
    "Also checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trained model to gs://cloud-training-demos-ml/flights/trained_model\n"
     ]
    }
   ],
   "source": [
    "model_dir='gs://{}/flights/trained_model'.format(BUCKET)\n",
    "os.environ['OUTDIR'] = model_dir  # needed for deployment\n",
    "print('Writing trained model to {}'.format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -rf $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0315 21:21:26.647607 139710232544640 deprecation.py:323] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0315 21:21:26.742827 139710232544640 deprecation.py:506] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0315 21:21:27.104131 139710232544640 deprecation.py:506] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0315 21:21:56.104225 139710232544640 deprecation.py:323] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DNNLinearCombinedClassifierV2' object has no attribute 'export_savedmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-71060725e1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mexporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLatestExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exporter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserving_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0meval_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexporters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    471\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    612\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1171\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1172\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1450\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m       logging.warning('Training with estimator made no steps. '\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0mexception_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m     \u001b[0;31m# __exit__ should return True to suppress an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexception_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_close_internal\u001b[0;34m(self, exception_type)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m           \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mend\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listeners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, session, step)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listeners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         logging.info(\n\u001b[1;32m    609\u001b[0m             \u001b[0;34m\"A CheckpointSaverListener requested that training be stopped. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mafter_save\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    517\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_trigger_for_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# updates self.eval_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_continuous_eval_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         logging.info('Exiting evaluation, as requested by '\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, global_step_value)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     self.eval_result, self.export_results = (\n\u001b[0;32m--> 539\u001b[0;31m         self._evaluator.evaluate_and_export())\n\u001b[0m\u001b[1;32m    540\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_EvalStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVALUATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m       \u001b[0;31m#  This is unexpected; should never happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mevaluate_and_export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m           self._max_training_steps if self._max_training_steps else False)\n\u001b[1;32m    926\u001b[0m       export_results = self._export_eval_result(eval_result,\n\u001b[0;32m--> 927\u001b[0;31m                                                 is_the_final_export)\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_the_final_export\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36m_export_eval_result\u001b[0;34m(self, eval_result, is_the_final_export)\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0meval_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                 is_the_final_export=is_the_final_export))\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mexport_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/exporter.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, estimator, export_path, checkpoint_path, eval_result, is_the_final_export)\u001b[0m\n\u001b[1;32m    472\u001b[0m     export_result = self._saved_model_exporter.export(\n\u001b[1;32m    473\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         is_the_final_export)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collect_exports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/exporter.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mis_the_final_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     export_result = estimator.export_savedmodel(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mexport_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serving_input_receiver_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DNNLinearCombinedClassifierV2' object has no attribute 'export_savedmodel'"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = sparse.values(),\n",
    "        dnn_feature_columns = real.values(),\n",
    "        dnn_hidden_units = [64, 32])\n",
    "\n",
    "train_batch_size = 64\n",
    "train_input_fn = lambda: prepare_dataset(TRAIN_DATA_PATTERN, train_batch_size)\n",
    "eval_batch_size = 100 if DEVELOP_MODE else 10000\n",
    "eval_input_fn = lambda: prepare_dataset(VALID_DATA_PATTERN, eval_batch_size, eval_batch_size*10, tf.estimator.ModeKeys.EVAL)\n",
    "num_steps = 10 if DEVELOP_MODE else (1000000 // train_batch_size)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps = num_steps)\n",
    "exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input_fn, steps=10, exporters=exporter)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo $model_dir\n",
    "saved_model_cli show --dir ${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"flights\"\n",
    "MODEL_VERSION=\"kfp\"\n",
    "TFVERSION=\"2.0\"\n",
    "MODEL_LOCATION=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#yes | gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine predict --model=flights --version=kfp --json-instances=example_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
